{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArslanNustian116/HLS4ML_Pycon2024/blob/main/ML_Workshop_Stuff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzU-6vcYN6sY"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUzrQzCEQ2dA"
      },
      "source": [
        "# Fetch Jet Tagging Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcAkAsADPJPz",
        "outputId": "588f5591-f2f3-4010-8950-609cb2a40fa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['zlogz', 'c1_b0_mmdt', 'c1_b1_mmdt', 'c1_b2_mmdt', 'c2_b1_mmdt', 'c2_b2_mmdt', 'd2_b1_mmdt', 'd2_b2_mmdt', 'd2_a1_b1_mmdt', 'd2_a1_b2_mmdt', 'm2_b1_mmdt', 'm2_b2_mmdt', 'n2_b1_mmdt', 'n2_b2_mmdt', 'mass_mmdt', 'multiplicity']\n",
            "(830000, 16) (830000,)\n",
            "      zlogz  c1_b0_mmdt  c1_b1_mmdt  c1_b2_mmdt  c2_b1_mmdt  c2_b2_mmdt  \\\n",
            "0 -2.935125    0.383155    0.005126    0.000084    0.009070    0.000179   \n",
            "1 -1.927335    0.270699    0.001585    0.000011    0.003232    0.000029   \n",
            "2 -3.112147    0.458171    0.097914    0.028588    0.124278    0.038487   \n",
            "3 -2.666515    0.437068    0.049122    0.007978    0.047477    0.004802   \n",
            "4 -2.484843    0.428981    0.041786    0.006110    0.023066    0.001123   \n",
            "\n",
            "   d2_b1_mmdt  d2_b2_mmdt  d2_a1_b1_mmdt  d2_a1_b2_mmdt  m2_b1_mmdt  \\\n",
            "0    1.769445    2.123898       1.769445       0.308185    0.135687   \n",
            "1    2.038834    2.563099       2.038834       0.211886    0.063729   \n",
            "2    1.269254    1.346238       1.269254       0.246488    0.115636   \n",
            "3    0.966505    0.601864       0.966505       0.160756    0.082196   \n",
            "4    0.552002    0.183821       0.552002       0.084338    0.048006   \n",
            "\n",
            "   m2_b2_mmdt  n2_b1_mmdt  n2_b2_mmdt   mass_mmdt  multiplicity  \n",
            "0    0.083278    0.412136    0.299058    8.926882          75.0  \n",
            "1    0.036310    0.310217    0.226661    3.886512          31.0  \n",
            "2    0.079094    0.357559    0.289220  162.144669          61.0  \n",
            "3    0.033311    0.238871    0.094516   91.258934          39.0  \n",
            "4    0.014450    0.141906    0.036665   79.725777          35.0  \n",
            "0    g\n",
            "1    w\n",
            "2    t\n",
            "3    z\n",
            "4    w\n",
            "Name: class, dtype: category\n",
            "Categories (5, object): ['g', 'q', 't', 'w', 'z']\n",
            "[[1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0.]]\n"
          ]
        }
      ],
      "source": [
        "data = fetch_openml('hls4ml_lhc_jets_hlf')\n",
        "X, y = data['data'], data['target']\n",
        "\n",
        "print(data['feature_names'])\n",
        "print(X.shape, y.shape)\n",
        "print(X[:5])\n",
        "print(y[:5])\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "y = to_categorical(y, 5)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(y[:5])\n",
        "scaler = StandardScaler()\n",
        "X_train_val = scaler.fit_transform(X_train_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "np.save('X_train_val.npy', X_train_val)\n",
        "np.save('X_test.npy', X_test)\n",
        "np.save('y_train_val.npy', y_train_val)\n",
        "np.save('y_test.npy', y_test)\n",
        "np.save('classes.npy', le.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5fOcZmZOD1o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "931781e9-79b4-4ecb-b486-ad62ca408ccb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'callbacks'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-46adbb5a040c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'X_train_val.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'X_test.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_train_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y_train_val.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'callbacks'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import callbacks\n",
        "\n",
        "X_train_val = np.load('X_train_val.npy')\n",
        "X_test = np.load('X_test.npy')\n",
        "y_train_val = np.load('y_train_val.npy')\n",
        "y_test = np.load('y_test.npy')\n",
        "classes = np.load('classes.npy', allow_pickle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNS3TkDRRBHK"
      },
      "source": [
        "# Define a Base Line Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMpS5EAHORzf"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from callbacks import all_callbacks\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_shape=(16,), name='fc1', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
        "model.add(Activation(activation='relu', name='relu1'))\n",
        "model.add(Dense(32, name='fc2', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
        "model.add(Activation(activation='relu', name='relu2'))\n",
        "model.add(Dense(32, name='fc3', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
        "model.add(Activation(activation='relu', name='relu3'))\n",
        "model.add(Dense(5, name='output', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
        "model.add(Activation(activation='softmax', name='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoyThhXqRWFI"
      },
      "source": [
        "# Train the Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3acRclO2RZdv"
      },
      "outputs": [],
      "source": [
        "train = True\n",
        "if train:\n",
        "    adam = Adam(lr=0.0001)\n",
        "    model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
        "    callbacks = all_callbacks(\n",
        "        stop_patience=1000,\n",
        "        lr_factor=0.5,\n",
        "        lr_patience=10,\n",
        "        lr_epsilon=0.000001,\n",
        "        lr_cooldown=2,\n",
        "        lr_minimum=0.0000001,\n",
        "        outputDir='model_1',\n",
        "    )\n",
        "    model.fit(\n",
        "        X_train_val,\n",
        "        y_train_val,\n",
        "        batch_size=1024,\n",
        "        epochs=30,\n",
        "        validation_split=0.25,\n",
        "        shuffle=True,\n",
        "        callbacks=callbacks.callbacks,\n",
        "    )\n",
        "else:\n",
        "    from tensorflow.keras.models import load_model\n",
        "\n",
        "    model = load_model('model_1/KERAS_check_best_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaG1l2iKQy8W"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow-model-optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X819j75bRG5q"
      },
      "source": [
        "# Setting Pruning Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQNY0l10OS7a"
      },
      "outputs": [],
      "source": [
        "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
        "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
        "\n",
        "pruning_params = {\"pruning_schedule\": pruning_schedule.ConstantSparsity(0.75, begin_step=2000, frequency=100)}\n",
        "model = prune.prune_low_magnitude(model, **pruning_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPrrf2KgRc8H"
      },
      "source": [
        "# Train Pruned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ICX1429OXyC"
      },
      "outputs": [],
      "source": [
        "train = True\n",
        "if train:\n",
        "    adam = Adam(lr=0.0001)\n",
        "    model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
        "    callbacks = all_callbacks(\n",
        "        stop_patience=1000,\n",
        "        lr_factor=0.5,\n",
        "        lr_patience=10,\n",
        "        lr_epsilon=0.000001,\n",
        "        lr_cooldown=2,\n",
        "        lr_minimum=0.0000001,\n",
        "        outputDir='model_2',\n",
        "    )\n",
        "    callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
        "    model.fit(\n",
        "        X_train_val,\n",
        "        y_train_val,\n",
        "        batch_size=1024,\n",
        "        epochs=30,\n",
        "        validation_split=0.25,\n",
        "        shuffle=True,\n",
        "        callbacks=callbacks.callbacks,\n",
        "    )\n",
        "    # Save the model again but with the pruning 'stripped' to use the regular layer types\n",
        "    model = strip_pruning(model)\n",
        "    model.save('model_2/KERAS_check_best_model.h5')\n",
        "else:\n",
        "    from tensorflow.keras.models import load_model\n",
        "\n",
        "    model = load_model('model_2/KERAS_check_best_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01AI8aJYSGo_"
      },
      "source": [
        "# Observing the Weights after Pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zt9m32nPUszo"
      },
      "outputs": [],
      "source": [
        "w = model.layers[0].weights[0].numpy()\n",
        "h, b = np.histogram(w, bins=100)\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.bar(b[:-1], h, width=b[1] - b[0])\n",
        "plt.semilogy()\n",
        "print('% of zeros = {}'.format(np.sum(w == 0) / np.size(w)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxJzNxFkSSER"
      },
      "source": [
        "# Compare Accuracy of Baseline and Pruned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBRkM6XSWjpB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model_ref = load_model('model_1/KERAS_check_best_model.h5')\n",
        "\n",
        "y_ref = model_ref.predict(X_test)\n",
        "y_prune = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy unpruned: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_ref, axis=1))))\n",
        "print(\"Accuracy pruned:   {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_prune, axis=1))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP7RXnhoSbV2"
      },
      "source": [
        "# Create HLS Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98yQsKZtZtYA"
      },
      "outputs": [],
      "source": [
        "!pip install hls4ml\n",
        "\n",
        "import hls4ml\n",
        "\n",
        "config = hls4ml.utils.config_from_keras_model(model, granularity='model')\n",
        "print(config)\n",
        "hls_model = hls4ml.converters.convert_from_keras_model(\n",
        "    model, hls_config=config, output_dir='model_2/hls4ml_prj', part='xcu250-figd2104-2L-e'\n",
        ")\n",
        "hls_model.compile()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jweahza-SrK9"
      },
      "source": [
        "# Create Quantized Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klvY2Omyc-QZ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from callbacks import all_callbacks\n",
        "from tensorflow.keras.layers import Activation\n",
        "from qkeras.qlayers import QDense, QActivation\n",
        "from qkeras.quantizers import quantized_bits, quantized_relu\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(\n",
        "    QDense(\n",
        "        64,\n",
        "        input_shape=(16,),\n",
        "        name='fc1',\n",
        "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
        "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
        "        kernel_initializer='lecun_uniform',\n",
        "        kernel_regularizer=l1(0.0001),\n",
        "    )\n",
        ")\n",
        "model.add(QActivation(activation=quantized_relu(6), name='relu1'))\n",
        "model.add(\n",
        "    QDense(\n",
        "        32,\n",
        "        name='fc2',\n",
        "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
        "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
        "        kernel_initializer='lecun_uniform',\n",
        "        kernel_regularizer=l1(0.0001),\n",
        "    )\n",
        ")\n",
        "model.add(QActivation(activation=quantized_relu(6), name='relu2'))\n",
        "model.add(\n",
        "    QDense(\n",
        "        32,\n",
        "        name='fc3',\n",
        "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
        "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
        "        kernel_initializer='lecun_uniform',\n",
        "        kernel_regularizer=l1(0.0001),\n",
        "    )\n",
        ")\n",
        "model.add(QActivation(activation=quantized_relu(6), name='relu3'))\n",
        "model.add(\n",
        "    QDense(\n",
        "        5,\n",
        "        name='output',\n",
        "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
        "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
        "        kernel_initializer='lecun_uniform',\n",
        "        kernel_regularizer=l1(0.0001),\n",
        "    )\n",
        ")\n",
        "model.add(Activation(activation='softmax', name='softmax'))\n",
        "\n",
        "\n",
        "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
        "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
        "\n",
        "pruning_params = {\"pruning_schedule\": pruning_schedule.ConstantSparsity(0.75, begin_step=2000, frequency=100)}\n",
        "model = prune.prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "train = True\n",
        "if train:\n",
        "    adam = Adam(lr=0.0001)\n",
        "    model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
        "    callbacks = all_callbacks(\n",
        "        stop_patience=1000,\n",
        "        lr_factor=0.5,\n",
        "        lr_patience=10,\n",
        "        lr_epsilon=0.000001,\n",
        "        lr_cooldown=2,\n",
        "        lr_minimum=0.0000001,\n",
        "        outputDir='model_3',\n",
        "    )\n",
        "    callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
        "    model.fit(\n",
        "        X_train_val,\n",
        "        y_train_val,\n",
        "        batch_size=1024,\n",
        "        epochs=30,\n",
        "        validation_split=0.25,\n",
        "        shuffle=True,\n",
        "        callbacks=callbacks.callbacks,\n",
        "    )\n",
        "    # Save the model again but with the pruning 'stripped' to use the regular layer types\n",
        "    model = strip_pruning(model)\n",
        "    model.save('model_3/KERAS_check_best_model.h5')\n",
        "else:\n",
        "    from tensorflow.keras.models import load_model\n",
        "    from qkeras.utils import _add_supported_quantized_objects\n",
        "\n",
        "    co = {}\n",
        "    _add_supported_quantized_objects(co)\n",
        "    model = load_model('model_3/KERAS_check_best_model.h5', custom_objects=co)\n",
        "\n",
        "\n",
        "import hls4ml\n",
        "\n",
        "config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
        "config['LayerName']['softmax']['exp_table_t'] = 'ap_fixed<18,8>'\n",
        "config['LayerName']['softmax']['inv_table_t'] = 'ap_fixed<18,4>'\n",
        "print(\"-----------------------------------\")\n",
        "hls_model = hls4ml.converters.convert_from_keras_model(\n",
        "    model, hls_config=config, output_dir='model_3/hls4ml_prj', part='xcu250-figd2104-2L-e'\n",
        ")\n",
        "hls_model.compile()\n",
        "\n",
        "y_qkeras = model.predict(np.ascontiguousarray(X_test))\n",
        "y_hls = hls_model.predict(np.ascontiguousarray(X_test))\n",
        "np.save('model_3/y_qkeras.npy', y_qkeras)\n",
        "np.save('model_3/y_hls.npy', y_hls)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DB5DEv5pg2Cp"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model_ref = load_model('model_1/KERAS_check_best_model.h5')\n",
        "y_ref = model_ref.predict(X_test)\n",
        "\n",
        "print(\"Accuracy baseline:  {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_ref, axis=1))))\n",
        "print(\"Accuracy pruned, quantized: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_qkeras, axis=1))))\n",
        "print(\"Accuracy hls4ml: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_hls, axis=1))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IO9AqdFMEUWA"
      },
      "outputs": [],
      "source": [
        "model1 = Sequential()\n",
        "model1.add(\n",
        "    QDense(\n",
        "        64,\n",
        "        input_shape=(16,),\n",
        "        name='fc1',\n",
        "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
        "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
        "        kernel_initializer='lecun_uniform',\n",
        "        kernel_regularizer=l1(0.0001),\n",
        "    )\n",
        ")\n",
        "model1.add(QActivation(activation=quantized_relu(6), name='relu1'))\n",
        "model1.add(\n",
        "    QDense(\n",
        "        32,\n",
        "        name='fc2',\n",
        "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
        "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
        "        kernel_initializer='lecun_uniform',\n",
        "        kernel_regularizer=l1(0.0001),\n",
        "    )\n",
        ")\n",
        "model1.add(QActivation(activation=quantized_relu(6), name='relu2'))\n",
        "model1.add(\n",
        "    QDense(\n",
        "        32,\n",
        "        name='fc3',\n",
        "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
        "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
        "        kernel_initializer='lecun_uniform',\n",
        "        kernel_regularizer=l1(0.0001),\n",
        "    )\n",
        ")\n",
        "model1.add(QActivation(activation=quantized_relu(6), name='relu3'))\n",
        "model1.add(\n",
        "    QDense(\n",
        "        5,\n",
        "        name='output',\n",
        "        kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
        "        bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
        "        kernel_initializer='lecun_uniform',\n",
        "        kernel_regularizer=l1(0.0001),\n",
        "    )\n",
        ")\n",
        "model1.add(Activation(activation='softmax', name='softmax'))\n",
        "\n",
        "\n",
        "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
        "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
        "\n",
        "pruning_params = {\"pruning_schedule\": pruning_schedule.ConstantSparsity(0.75, begin_step=2000, frequency=100)}\n",
        "model1 = prune.prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "train = True\n",
        "if train:\n",
        "    adam = Adam(lr=0.0001)\n",
        "    model1.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
        "    callbacks = all_callbacks(\n",
        "        stop_patience=1000,\n",
        "        lr_factor=0.5,\n",
        "        lr_patience=10,\n",
        "        lr_epsilon=0.000001,\n",
        "        lr_cooldown=2,\n",
        "        lr_minimum=0.0000001,\n",
        "        outputDir='model_4',\n",
        "    )\n",
        "    callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
        "    model1.fit(\n",
        "        X_train_val,\n",
        "        y_train_val,\n",
        "        batch_size=1024,\n",
        "        epochs=30,\n",
        "        validation_split=0.25,\n",
        "        shuffle=True,\n",
        "        callbacks=callbacks.callbacks,\n",
        "    )\n",
        "    # Save the model again but with the pruning 'stripped' to use the regular layer types\n",
        "    model1 = strip_pruning(model)\n",
        "    model1.save('model_4/KERAS_check_best_model.h5')\n",
        "else:\n",
        "    from tensorflow.keras.models import load_model\n",
        "    from qkeras.utils import _add_supported_quantized_objects\n",
        "\n",
        "    co = {}\n",
        "    _add_supported_quantized_objects(co)\n",
        "    model1 = load_model('model_4/KERAS_check_best_model.h5', custom_objects=co)\n",
        "\n",
        "y_qkeras1 = model1.predict(np.ascontiguousarray(X_test))\n",
        "\n",
        "\n",
        "print(\"Accuracy pruned, quantized: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_qkeras, axis=1))))\n",
        "print(\"Accuracy quantized (new precision): {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_qkeras1, axis=1))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "js-hM7LrhChY"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy baseline:  {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_ref, axis=1))))\n",
        "print(\"Accuracy quantized (previous precision): ...\") # Add your previous accuracy result here\n",
        "print(\"Accuracy quantized (new precision): {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_qkeras, axis=1))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnhA67qpN-lG"
      },
      "outputs": [],
      "source": [
        "!pip install qkeras\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from sklearn.metrics import accuracy_score\n",
        "from qkeras import QDense, QActivation, quantized_bits, quantized_relu\n",
        "\n",
        "# Set your seed for reproducibility\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# Define a function to build your model\n",
        "def build_quantized_model(input_shape, bits):\n",
        "    model = Sequential()\n",
        "    model.add(QDense(64, input_shape=(input_shape,), name='fc1',\n",
        "                     kernel_quantizer=quantized_bits(bits, 0, alpha=1),\n",
        "                     bias_quantizer=quantized_bits(bits, 0, alpha=1),\n",
        "                     kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
        "    model.add(QActivation(activation=quantized_relu(bits), name='relu1'))\n",
        "    model.add(QDense(32, name='fc2',\n",
        "                     kernel_quantizer=quantized_bits(bits, 0, alpha=1),\n",
        "                     bias_quantizer=quantized_bits(bits, 0, alpha=1),\n",
        "                     kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
        "    model.add(QActivation(activation=quantized_relu(bits), name='relu2'))\n",
        "    model.add(QDense(32, name='fc3',\n",
        "                     kernel_quantizer=quantized_bits(bits, 0, alpha=1),\n",
        "                     bias_quantizer=quantized_bits(bits, 0, alpha=1),\n",
        "                     kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
        "    model.add(QActivation(activation=quantized_relu(bits), name='relu3'))\n",
        "    model.add(QDense(5, name='output',\n",
        "                     kernel_quantizer=quantized_bits(bits, 0, alpha=1),\n",
        "                     bias_quantizer=quantized_bits(bits, 0, alpha=1),\n",
        "                     kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
        "    model.add(Activation('softmax'))\n",
        "    return model\n",
        "\n",
        "# Assuming X_train_val, y_train_val, X_test, y_test are loaded and preprocessed\n",
        "\n",
        "bit_precisions = [2, 4, 6, 8, 10, 12, 16]\n",
        "accuracies = []\n",
        "\n",
        "for bits in bit_precisions:\n",
        "    model = build_quantized_model(16, bits)\n",
        "    model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train your model\n",
        "    model.fit(X_train_val, y_train_val, batch_size=1024, epochs=30, validation_split=0.25, shuffle=True)\n",
        "\n",
        "    # Evaluate your model\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = accuracy_score(np.argmax(y_test, axis=1), np.argmax(predictions, axis=1))\n",
        "\n",
        "    accuracies.append(accuracy)\n",
        "    print(f\"Accuracy with {bits}-bit precision: {accuracy}\")\n",
        "\n",
        "# Save accuracies to a NumPy array file for further analysis\n",
        "np.save('quantized_model_accuracies.npy', np.array(accuracies))\n",
        "\n",
        "# To check and compare accuracies\n",
        "loaded_accuracies = np.load('quantized_model_accuracies.npy')\n",
        "print(\"Loaded Accuracies:\", loaded_accuracies)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNQhsCjrbcWkHFPRQ/mggAo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}